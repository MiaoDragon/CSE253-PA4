{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataloader import create_split_loaders\n",
    "from model import GenericRNN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "import os\n",
    "from utility import *\n",
    "\n",
    "config = {'chunk_size':100, 'type_number':93, 'hidden':100, \n",
    "          'learning_rate':0.001, 'early_stop':True, 'increase_limit':3, \n",
    "          'epoch_num':1, 'N':50, 'M':100, 'seed':1, 'model':'LSTM', 'model_path':'model_weights'}\n",
    "\n",
    "def train(config):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        computing_device = torch.device(\"cuda\")\n",
    "        extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "        print(\"CUDA is supported\")\n",
    "    else: # Otherwise, train on the CPU\n",
    "        computing_device = torch.device(\"cpu\")\n",
    "        extras = False\n",
    "        print(\"CUDA NOT supported\")\n",
    "    \n",
    "    seed = config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # the size of every chunk\n",
    "    chunk_size = config['chunk_size']\n",
    "    # number of types, it is a constant\n",
    "    type_number = config['type_number']\n",
    "    # number of features in hidden layer\n",
    "    hidden = config['hidden']\n",
    "    # learning rate\n",
    "    learning_rate = config['learning_rate']\n",
    "    # whether we use early stop\n",
    "    early_stop = config['early_stop']\n",
    "    # after validation loss increase how many times do we stop training\n",
    "    increase_limit = config['increase_limit']\n",
    "    # number of epoch\n",
    "    epoch_num = config['epoch_num']\n",
    "    # receive train, validation, test data\n",
    "    train, valid, test, c_to, one_to = create_split_loaders(chunk_size,extras)\n",
    "    \n",
    "    # construct network\n",
    "    net = GenericRNN(type_number, hidden, type_number, config['model'])\n",
    "    # if model already exists, then load the previous one\n",
    "    if os.path.exists(config['model_path']+'.pkl'):\n",
    "        print('loading previous model...')\n",
    "        load_net_state(net, config['model_path']+'.pkl')\n",
    "    net = net.to(computing_device)\n",
    "    \n",
    "    # use cross entropy loss\n",
    "    criterion = nn.BCELoss()\n",
    "    # keep tracking of the traininig loss\n",
    "    training_record = []\n",
    "    # keep tracking of the validation loss\n",
    "    validation_record = []\n",
    "    \n",
    "    # Using Adam\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    # if model already exists, then load the previous optimizer state\n",
    "    prev_total_loss, prev_avg_minibatch_loss, prev_val_loss = [], [], []\n",
    "    if os.path.exists(config['model_path']+'.pkl'):\n",
    "        print('loading optimizer state...')\n",
    "        load_opt_state(optimizer, config['model_path']+'.pkl')\n",
    "        # notice when saving prev_val_loss, we ignored the first val_loss\n",
    "        prev_total_loss, prev_avg_minibatch_loss, prev_val_loss = load_loss(config['model_path']+'.pkl')\n",
    "    \n",
    "    total_loss = [] + prev_total_loss\n",
    "    avg_minibatch_loss = [] + prev_avg_minibatch_loss\n",
    "    val_loss = [1000000000] + prev_val_loss #assume large error at the begining\n",
    "\n",
    "    last_valid = float('inf')\n",
    "    best_net = -1\n",
    "    # after how many batches do we record the training loss\n",
    "    N = config['N']\n",
    "    # after how many batches do we examine whether validation loss increase\n",
    "    M = config['M']\n",
    "    # store best loss\n",
    "    best_loss = float('inf')\n",
    "    increasement = 0\n",
    "    for epoch in range(epoch_num):\n",
    "        count = 0\n",
    "        average_loss = 0\n",
    "        state_0 = None\n",
    "        for minibatch in train:\n",
    "            count += 1\n",
    "            if minibatch[0].size()[0] != chunk_size:\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            train_batch = minibatch[0]\n",
    "            target_batch = minibatch[1]\n",
    "            train_batch = train_batch.to(computing_device)\n",
    "            target_batch = target_batch.to(computing_device)\n",
    "            predict_batch, state_0 = net(train_batch, state_0)\n",
    "            if isinstance(state_0, tuple):\n",
    "                state_0 = list(state_0)\n",
    "                for i in range(len(state_0)):\n",
    "                    state_0[i] = state_0[i].detach()\n",
    "                state_0 = tuple(state_0)\n",
    "            else:\n",
    "                state_0 = state_0.detach()\n",
    "            loss = criterion(predict_batch, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "            average_loss += loss.item()\n",
    "            if count % N == 0:\n",
    "                training_record.append(average_loss / N)\n",
    "                avg_minibatch_loss.append(average_loss / N)\n",
    "                average_loss = 0\n",
    "                print('keep tracking of training error:')\n",
    "                print(training_record)\n",
    "            # validation \n",
    "            if count % M == 0:\n",
    "                with torch.no_grad():\n",
    "                    loss_val = 0\n",
    "                    count_val = 0\n",
    "                    state_0 = None\n",
    "                    for val in valid:\n",
    "                        count_val += 1\n",
    "                        if val[0].size()[0] != chunk_size:\n",
    "                            break\n",
    "                        valid_batch = val[0].to(computing_device)\n",
    "                        valid_target = val[1].to(computing_device)\n",
    "                        valid_predict, state_0 = net(valid_batch, state_0)\n",
    "                        loss_val += criterion(valid_predict, valid_target)\n",
    "                    loss_val /= count_val\n",
    "                    val_loss.append(loss_val.item())\n",
    "                    validation_record.append(loss_val.item())\n",
    "                    print('keep tracking of validation error')\n",
    "                    print(validation_record)\n",
    "                    if loss_val < best_loss:\n",
    "                        print('best model is updated')\n",
    "                        best_loss = loss_val\n",
    "                        best_net = copy.deepcopy(net)\n",
    "                save_state(best_net, optimizer, total_loss, avg_minibatch_loss, val_loss[1:], \\\n",
    "                           seed, config['model_path']+'.pkl')\n",
    "                if early_stop:\n",
    "                    if loss_val > last_valid:\n",
    "                        increasement += 1\n",
    "                    else:\n",
    "                        increasement = 0\n",
    "                    last_valid = loss_val\n",
    "                    if increasement >= increase_limit:\n",
    "                        break\n",
    "        if early_stop:\n",
    "            if increasement >= increase_limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NOT supported\n",
      "loading previous model...\n",
      "loading optimizer state...\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858]\n",
      "best model is updated\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858, 0.047383248805999756]\n",
      "best model is updated\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858, 0.047383248805999756, 0.04709471017122269]\n",
      "best model is updated\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858, 0.047383248805999756, 0.04709471017122269, 0.04592517763376236]\n",
      "best model is updated\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858, 0.047383248805999756, 0.04709471017122269, 0.04592517763376236, 0.04838724061846733]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987, 0.03388229198753834]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987, 0.03388229198753834, 0.03670437354594469]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858, 0.047383248805999756, 0.04709471017122269, 0.04592517763376236, 0.04838724061846733, 0.04619288817048073]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987, 0.03388229198753834, 0.03670437354594469, 0.034063123129308225]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987, 0.03388229198753834, 0.03670437354594469, 0.034063123129308225, 0.02966598156839609]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858, 0.047383248805999756, 0.04709471017122269, 0.04592517763376236, 0.04838724061846733, 0.04619288817048073, 0.04795962944626808]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987, 0.03388229198753834, 0.03670437354594469, 0.034063123129308225, 0.02966598156839609, 0.027513810824602844]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987, 0.03388229198753834, 0.03670437354594469, 0.034063123129308225, 0.02966598156839609, 0.027513810824602844, 0.029797771386802195]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858, 0.047383248805999756, 0.04709471017122269, 0.04592517763376236, 0.04838724061846733, 0.04619288817048073, 0.04795962944626808, 0.048868969082832336]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987, 0.03388229198753834, 0.03670437354594469, 0.034063123129308225, 0.02966598156839609, 0.027513810824602844, 0.029797771386802195, 0.02816617291420698]\n",
      "keep tracking of training error:\n",
      "[0.04477706223726272, 0.03850761767476797, 0.04147551707923412, 0.03873007643967867, 0.03543800260871649, 0.0375898901745677, 0.03741276018321514, 0.036208095848560336, 0.03362639289349317, 0.02936684910207987, 0.03388229198753834, 0.03670437354594469, 0.034063123129308225, 0.02966598156839609, 0.027513810824602844, 0.029797771386802195, 0.02816617291420698, 0.028715513572096826]\n",
      "keep tracking of validation error\n",
      "[0.04889550805091858, 0.047383248805999756, 0.04709471017122269, 0.04592517763376236, 0.04838724061846733, 0.04619288817048073, 0.04795962944626808, 0.048868969082832336, 0.05080417916178703]\n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
