{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataloader import create_split_loaders\n",
    "from model import GenericRNN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "import os\n",
    "from utility import *\n",
    "\n",
    "config = {'chunk_size':100, 'type_number':93, 'hidden':100, \n",
    "          'learning_rate':0.001, 'early_stop':True, 'increase_limit':3, \n",
    "          'epoch_num':1, 'N':50, 'M':100, 'seed':1, 'model':'LSTM', 'model_path':'model_weights'}\n",
    "\n",
    "def train(config):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        computing_device = torch.device(\"cuda\")\n",
    "        extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "        print(\"CUDA is supported\")\n",
    "    else: # Otherwise, train on the CPU\n",
    "        computing_device = torch.device(\"cpu\")\n",
    "        extras = False\n",
    "        print(\"CUDA NOT supported\")\n",
    "    \n",
    "    seed = config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # the size of every chunk\n",
    "    chunk_size = config['chunk_size']\n",
    "    # number of types, it is a constant\n",
    "    type_number = config['type_number']\n",
    "    # number of features in hidden layer\n",
    "    hidden = config['hidden']\n",
    "    # learning rate\n",
    "    learning_rate = config['learning_rate']\n",
    "    # whether we use early stop\n",
    "    early_stop = config['early_stop']\n",
    "    # after validation loss increase how many times do we stop training\n",
    "    increase_limit = config['increase_limit']\n",
    "    # number of epoch\n",
    "    epoch_num = config['epoch_num']\n",
    "    # receive train, validation, test data\n",
    "    train, valid, test, c_to, one_to = create_split_loaders(chunk_size,extras)\n",
    "    \n",
    "    # construct network\n",
    "    net = GenericRNN(type_number, hidden, type_number, config['model'])\n",
    "    # if model already exists, then load the previous one\n",
    "    if os.path.exists(config['model_path']+'.pkl'):\n",
    "        print('loading previous model...')\n",
    "        load_net_state(net, config['model_path']+'.pkl')\n",
    "    net = net.to(computing_device)\n",
    "    \n",
    "    # use cross entropy loss\n",
    "    criterion = nn.BCELoss()\n",
    "    # keep tracking of the traininig loss\n",
    "    training_record = []\n",
    "    # keep tracking of the validation loss\n",
    "    validation_record = []\n",
    "    \n",
    "    # Using Adam\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    # if model already exists, then load the previous optimizer state\n",
    "    prev_total_loss, prev_avg_minibatch_loss, prev_val_loss = [], [], []\n",
    "    if os.path.exists(config['model_path']+'.pkl'):\n",
    "        print('loading optimizer state...')\n",
    "        load_opt_state(optimizer, config['model_path']+'.pkl')\n",
    "        # notice when saving prev_val_loss, we ignored the first val_loss\n",
    "        prev_total_loss, prev_avg_minibatch_loss, prev_val_loss = load_loss(config['model_path']+'.pkl')\n",
    "    \n",
    "    total_loss = [] + prev_total_loss\n",
    "    avg_minibatch_loss = [] + prev_avg_minibatch_loss\n",
    "    val_loss = [1000000000] + prev_val_loss #assume large error at the begining\n",
    "\n",
    "    last_valid = float('inf')\n",
    "    best_net = -1\n",
    "    # after how many batches do we record the training loss\n",
    "    N = config['N']\n",
    "    # after how many batches do we examine whether validation loss increase\n",
    "    M = config['M']\n",
    "    # store best loss\n",
    "    best_loss = float('inf')\n",
    "    increasement = 0\n",
    "    for epoch in range(epoch_num):\n",
    "        count = 0\n",
    "        average_loss = 0\n",
    "        state_0 = None\n",
    "        for minibatch in train:\n",
    "            count += 1\n",
    "            if minibatch[0].size()[0] != chunk_size:\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            train_batch = minibatch[0]\n",
    "            target_batch = minibatch[1]\n",
    "            train_batch = train_batch.to(computing_device)\n",
    "            target_batch = target_batch.to(computing_device)\n",
    "            predict_batch, state_0 = net(train_batch, state_0)\n",
    "            if isinstance(state_0, tuple):\n",
    "                state_0 = list(state_0)\n",
    "                for i in range(len(state_0)):\n",
    "                    state_0[i] = state_0[i].detach()\n",
    "                state_0 = tuple(state_0)\n",
    "            else:\n",
    "                state_0 = state_0.detach()\n",
    "            loss = criterion(predict_batch, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "            average_loss += loss.item()\n",
    "            if count % N == 0:\n",
    "                training_record.append(average_loss / N)\n",
    "                avg_minibatch_loss.append(average_loss / N)\n",
    "                average_loss = 0\n",
    "                print('keep tracking of training error:')\n",
    "                print(training_record)\n",
    "            # validation \n",
    "            if count % M == 0:\n",
    "                with torch.no_grad():\n",
    "                    loss_val = 0\n",
    "                    count_val = 0\n",
    "                    state_0 = None\n",
    "                    for val in valid:\n",
    "                        count_val += 1\n",
    "                        if val[0].size()[0] != chunk_size:\n",
    "                            break\n",
    "                        valid_batch = val[0].to(computing_device)\n",
    "                        valid_target = val[1].to(computing_device)\n",
    "                        valid_predict, state_0 = net(valid_batch, state_0)\n",
    "                        loss_val += criterion(valid_predict, valid_target)\n",
    "                    loss_val /= count_val\n",
    "                    val_loss.append(loss_val.item())\n",
    "                    validation_record.append(loss_val.item())\n",
    "                    print('keep tracking of validation error')\n",
    "                    print(validation_record)\n",
    "                    if loss_val < best_loss:\n",
    "                        print('best model is updated')\n",
    "                        best_loss = loss_val\n",
    "                        best_net = copy.deepcopy(net)\n",
    "                save_state(best_net, optimizer, total_loss, avg_minibatch_loss, val_loss[1:], \\\n",
    "                           seed, config['model_path']+'.pkl')\n",
    "                if early_stop:\n",
    "                    if loss_val > last_valid:\n",
    "                        increasement += 1\n",
    "                    else:\n",
    "                        increasement = 0\n",
    "                    last_valid = loss_val\n",
    "                    if increasement >= increase_limit:\n",
    "                        break\n",
    "        if early_stop:\n",
    "            if increasement >= increase_limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NOT supported\n",
      "loading previous model...\n",
      "loading optimizer state...\n",
      "(tensor([[[ 0.0423,  0.0787, -0.2632, -0.1260, -0.0251,  0.1976, -0.0886,\n",
      "           0.0176,  0.1433, -0.3815, -0.0683, -0.1569,  0.5454,  0.0738,\n",
      "          -0.0885, -0.0777, -0.3403,  0.1295, -0.0046,  0.2036,  0.3366,\n",
      "           0.2578, -0.1605, -0.1738,  0.3209, -0.3033,  0.1296,  0.0457,\n",
      "          -0.1070,  0.2613,  0.1278, -0.3992, -0.2642,  0.2159, -0.2256,\n",
      "          -0.0019,  0.2205, -0.2139, -0.2122,  0.4216, -0.0386, -0.1011,\n",
      "          -0.1681, -0.1251,  0.0029,  0.3127, -0.0812,  0.3352, -0.4260,\n",
      "          -0.0302, -0.1462,  0.2002,  0.1835, -0.0688,  0.2053, -0.0123,\n",
      "           0.1730,  0.3783, -0.4976,  0.0284, -0.2557,  0.0777, -0.0990,\n",
      "           0.0595, -0.2728,  0.2201, -0.1350, -0.1846,  0.2042, -0.3683,\n",
      "           0.0599,  0.0124, -0.0125, -0.1825, -0.2033, -0.1356,  0.4713,\n",
      "           0.1615, -0.1527,  0.0713, -0.2308, -0.1369,  0.3770, -0.2099,\n",
      "          -0.1120, -0.2216,  0.1475, -0.2688, -0.0538,  0.2063, -0.1175,\n",
      "           0.0397, -0.3397,  0.6560, -0.5201, -0.1139,  0.0475, -0.0471,\n",
      "           0.4910, -0.3286]]], grad_fn=<ViewBackward>), tensor([[[ 0.0701,  0.1405, -0.4130, -0.2087, -0.0411,  0.3732, -0.1425,\n",
      "           0.0238,  0.2061, -0.6106, -0.1309, -0.3061,  0.8374,  0.1323,\n",
      "          -0.1422, -0.1269, -0.5555,  0.2460, -0.0083,  0.4032,  0.5527,\n",
      "           0.4726, -0.2778, -0.3063,  0.5368, -0.5055,  0.1791,  0.0780,\n",
      "          -0.1786,  0.3299,  0.2210, -0.6676, -0.4476,  0.3355, -0.3453,\n",
      "          -0.0032,  0.3842, -0.3152, -0.3850,  0.6236, -0.0648, -0.1731,\n",
      "          -0.2504, -0.2184,  0.0056,  0.4710, -0.0978,  0.5895, -0.6144,\n",
      "          -0.0454, -0.2736,  0.3568,  0.2579, -0.1214,  0.3467, -0.0175,\n",
      "           0.3251,  0.5649, -0.7748,  0.0491, -0.3363,  0.1118, -0.1592,\n",
      "           0.1033, -0.5039,  0.3237, -0.3115, -0.2836,  0.3681, -0.6461,\n",
      "           0.0977,  0.0179, -0.0214, -0.3341, -0.4020, -0.1759,  0.6580,\n",
      "           0.2789, -0.2631,  0.1224, -0.3914, -0.2138,  0.5936, -0.3474,\n",
      "          -0.1344, -0.3854,  0.2276, -0.5025, -0.0828,  0.4181, -0.2142,\n",
      "           0.0740, -0.5090,  1.0913, -0.9044, -0.1865,  0.0819, -0.0838,\n",
      "           0.7580, -0.6519]]], grad_fn=<ViewBackward>))\n",
      "(tensor([[[-0.0569,  0.0041, -0.0810, -0.1205, -0.1865,  0.1638, -0.1921,\n",
      "          -0.1345,  0.0231, -0.2926, -0.1565, -0.1981,  0.2722,  0.1299,\n",
      "          -0.0539, -0.1433, -0.2093,  0.1050, -0.0292,  0.1258,  0.2713,\n",
      "           0.1581, -0.2190, -0.1666,  0.1713, -0.0835,  0.0020,  0.0579,\n",
      "          -0.2407,  0.0632,  0.0360, -0.2081, -0.2754,  0.1516, -0.1184,\n",
      "          -0.1074,  0.2211, -0.0547, -0.1554,  0.2258,  0.1451, -0.0706,\n",
      "          -0.2644, -0.1714,  0.1134,  0.0747, -0.2290,  0.1974, -0.1407,\n",
      "           0.1010, -0.1246,  0.1875, -0.0237, -0.0551,  0.1478,  0.0659,\n",
      "           0.1751,  0.0871, -0.1876,  0.1825, -0.3744, -0.0840,  0.0222,\n",
      "           0.0032, -0.1681, -0.0336, -0.1696, -0.1427,  0.1803, -0.1947,\n",
      "           0.2365, -0.0661, -0.1668, -0.1003, -0.1292, -0.2948,  0.0600,\n",
      "           0.0672, -0.1355,  0.0926, -0.3126, -0.1525,  0.1297, -0.0744,\n",
      "           0.2047, -0.1444, -0.0920, -0.2118, -0.1504,  0.1279, -0.2084,\n",
      "           0.0764, -0.3754,  0.0500, -0.3257, -0.0842,  0.1337, -0.0158,\n",
      "           0.1880, -0.2995]]], grad_fn=<ViewBackward>), tensor([[[-0.0951,  0.0069, -0.1248, -0.1984, -0.3060,  0.2921, -0.3160,\n",
      "          -0.1746,  0.0360, -0.4802, -0.2673, -0.3793,  0.4087,  0.2140,\n",
      "          -0.0892, -0.2217, -0.3463,  0.1874, -0.0545,  0.2352,  0.4842,\n",
      "           0.2919, -0.3609, -0.2876,  0.2749, -0.1478,  0.0033,  0.1032,\n",
      "          -0.4033,  0.0869,  0.0632, -0.3503, -0.4737,  0.2571, -0.1806,\n",
      "          -0.1712,  0.3959, -0.0805, -0.2820,  0.3363,  0.2441, -0.1208,\n",
      "          -0.3993, -0.3024,  0.2021,  0.1151, -0.3210,  0.3539, -0.2008,\n",
      "           0.1547, -0.2211,  0.3266, -0.0372, -0.0948,  0.2459,  0.0989,\n",
      "           0.2946,  0.1409, -0.3055,  0.3389, -0.5725, -0.1281,  0.0397,\n",
      "           0.0053, -0.2916, -0.0535, -0.3610, -0.2422,  0.3017, -0.3094,\n",
      "           0.3813, -0.0977, -0.2835, -0.1844, -0.2539, -0.4132,  0.0803,\n",
      "           0.1066, -0.2223,  0.1560, -0.5360, -0.2450,  0.2067, -0.1268,\n",
      "           0.2817, -0.2421, -0.1470, -0.3666, -0.2398,  0.2322, -0.3791,\n",
      "           0.1340, -0.5931,  0.0643, -0.5284, -0.1445,  0.2295, -0.0266,\n",
      "           0.2863, -0.5725]]], grad_fn=<ViewBackward>))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8e44f5f578ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-e4f7ecd9ff51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
